{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_multinomial_logit",
      "provenance": [],
      "authorship_tag": "ABX9TyN1Gp7bicho+NTdBH+HV+XT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmontman/pub-choicemodels/blob/main/nb/03_multinomial_logit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfpPjNjRSta7"
      },
      "source": [
        "# Lecture 3: Multinomial logit\n",
        "\n",
        "We are introducing the multinomial logit, the applied model for choice modelling.\n",
        "\n",
        "# Recap\n",
        "\n",
        "The view as utility + random component\n",
        "\n",
        "Binary choice\n",
        "\n",
        "The view as predicting the probability\n",
        "\n",
        "The view as predicting the log-odds\n",
        "\n",
        "The view as random component having Gumbel distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCPwYFGDTlAd"
      },
      "source": [
        "# Multinomial\n",
        "\n",
        "We have more than two alternatives\n",
        "\n",
        "**Question:** When we have 2 alternatives, we only need to determine 1 of them, the other is completely dependent because of probability.  **How many of the alternatives have to be completely determined, when \n",
        " we have $J$ alternatives?**\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEraSDTeOasI"
      },
      "source": [
        "\n",
        "# Independence of Irrelevant Alternatives (IIA)\n",
        "\n",
        "When talking about utility, we have imposed some 'axioms' in the decision-making process such as transitivity, completeness (some of them are questionable).\n",
        "If we accept these axioms,  then we can show mathematically that any decision-making process can be represented by utility functions that assign a number to each alternative, and then the individuals acts as maximizing utility, they will choose the alternative that gives them higher utility. This make analysis easier and enables precise quantitative descriptions.\n",
        "\n",
        "Then we mentioned that we can impose additional restrictions on the decision-making process, for example,a very popular one is that there is a linear relationship between\n",
        " some variables in the process and the utility (attributes of the alternatives and characteristics of the individuals).\n",
        "\n",
        "There is another important axiom that is comes into play when we have to deal with more than two alternatives, and we will be able to almost completely derive the multinomial logit model from this axiom:\n",
        "\n",
        " **Independence of irrelevant alternatives**\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXmQGTzUOXnd"
      },
      "source": [
        "\n",
        "# Informal description of IIA\n",
        "\n",
        " Informally, it means that the preference between any two alternatives should only depend on those two alternatives, not on the remaining of the choice set.\n",
        " For example, we have alternatives A and B, say latte and espresso in the coffee shop example. Imagine that the individuals prefer latte to espresso.\n",
        " We now change the choice set, adding a third alternative C, long black. The independence of irrelevant alternatives states that the preference between latte and espresso should not change, it cannot happen that adding long black modifies the preference and makes the individuals prefer espresso to latte.\n",
        "\n",
        " This axiom seems quite reasonable, it appears in some form in many behavioral sciences, not only choice modelling (e.g. game theory). The IIA axiom, in addition to seeming reasonable at first glance, has a powerful consequence:\n",
        "\n",
        " * It allows us to compare preferences by pairs, without needing to consider the full choice set. For us, as analysts, this means that we can subdivide the problem up until the level of pairs. When we have many altenatives, think hundreds, we can present subsets of those alternatives at a time and still recover the equivalent result as if the subjects were presented the full choice set. This in fact might be the reason that it was proposed for choice modelling.\n",
        "\n",
        "\n",
        "The specific definition of the IIA axiom varies depending of the subfield, in choice modelling, it is constant probability ratio: The ratio of choice probabilties of any two alternatives is not affected by the presence of any other alternative. Note that the absolute probabilities might be affected, bt not the ration. If we recall our definitions of odds, it means that the odds are kept constant.\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onVmEmSLOkCj"
      },
      "source": [
        "\n",
        "\n",
        "# Formal definition of IIA in choice modelling\n",
        "\n",
        "It is formally defined as:\n",
        "\n",
        "For any choice set $X$, measured attributes $s$ and alternatives $a, b$ of $X$\n",
        "\n",
        "$$P(a | s, \\{a,b\\})P(b | s, X) = P(b, | s, \\{a,b\\})P(a | s, X)$$\n",
        "\n",
        "The notation $P(a | s, \\{a,b\\})$ stands for: the choice probability of alternative $a$ with attributes $s$ when the choice set is $\\{a,b\\}$,\n",
        "\n",
        "We can write a bit more informally, when the choice probabilities are nonzero as:\n",
        "\n",
        "$$ \\frac{P( a | \\{a,b\\})}{P(b | \\{a,b\\})} = \\frac{P(a | X)}{P(b | X)}$$\n",
        "\n",
        "And here it is clear what it is called the 'constant odds', the odds of any two alternatives do not change when we modify the choice set.\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99qZpz8EPt9E"
      },
      "source": [
        "\n",
        "# Deriving the multinomial logit from IIA\n",
        "\n",
        "From the IIA and the properties of probability we can derive the multinomial logit.\n",
        "\n",
        " * Probabilities are positive (even if very small)\n",
        " * Probabilities have to sum one\n",
        " * IIA \n",
        " * Log odds are linear on the variables (logistic regression)\n",
        "\n",
        "\n",
        "1. From the IIA and positive probabilities we get:\n",
        " $$P(b | X) = \\frac{P( b | \\{a,b\\})}{P(a | \\{a,b\\})} P(a | X)$$\n",
        "\n",
        "2. From probabilities have to sum 1. Assuming $P( a | \\{a,a\\}) = 0.5$\n",
        "\n",
        "$$ 1 = \\sum_{b \\in X}P(b | X) = \\sum_{b \\in X} \\left(\\frac{P( b | \\{a,b\\})}{P(a | \\{a,b\\})} \\right)P(a | X)$$\n",
        "\n",
        "\n",
        "3. We can now see that the probabilities in the full choice can be written in terms of probabilities among pairs of alternatives:\n",
        "\n",
        "$$ P(a | X) = \\frac{1}{\\sum_{b \\in X} \\left(\\frac{P( b | \\{a,b\\})}{P(a | \\{a,b\\})} \\right)}$$\n",
        "\n",
        "4. We can keep manipulating the equations, we can consider one of the alternatives in $X$ as the 'reference' or benchmark alternative, to rewrite everything in terms of binary comparisons with that alternative. If we call that alternative $c$, and multiply expression (3) by the binary probability with $c$, we would get:\n",
        "$$ P(a | X) = \\frac{ \\frac{P( a | \\{a,c\\})}{P( c | \\{a,c\\}) }}{\\sum_{b \\in X} \\left(\\frac{P( b | \\{b,c\\})}{P(c | \\{b,c\\})} \\right)}$$\n",
        "\n",
        "5. If we consider that the log-odds are linear, or that pairs can be modelled by the binary logit, recall: \n",
        "\n",
        "$$ \\log \\left( \\frac{P( b | \\{b,c\\})}{P(c | \\{b,c\\})}\\right) = V_b$$\n",
        "\n",
        "In this case $V_b$ is the notation for observed utility, a linear function on the variables $s$ measured for the binary choice problem involving $a$ and $c$. Just as we saw in last lecture, for example: $\\alpha \\text{Price} + \\beta \\text{Age}$\n",
        "\n",
        "So now if we introduce that the pairwise choices can be modelled by the logit, we will arrive to the expression of the multinomial logit:\n",
        "\n",
        "$$ P(a | X) = \\frac{ e^{V_a}}{\\sum_{b \\in X} e^{V_b}}$$\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "# Connection to binary choice, the logit\n",
        "\n",
        "When $X = \\{a,b\\}$, we can choose the third alternative $c$ here to be $b$ and show\n",
        "\n",
        "$$ P(a | X) = \\frac{ e^{V_a}}{ e^{V_a} + e^{V_b}}$$\n",
        "by the definition of $V_b$ when $b$ is the third alternative:\n",
        "\n",
        "$$ \\log \\left( \\frac{P( b | \\{b,b\\})}{P(b | \\{b,b\\})}\\right) = V_b = 1$$\n",
        "\n",
        "we get\n",
        "\n",
        "$$ P(a | X) = \\frac{ e^{V_a}}{ e^{V_a} + 1}$$ which is the definition of the logistic $S(x) = \\frac{e^x}{1 + e^x}$ from lecture 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5cp1cSSP7Wx"
      },
      "source": [
        "\n",
        "\n",
        "# Softmax view\n",
        "\n",
        "We saw that the logit squashes a linear model that can produce arbitrary numbers into the range [0,1] so the output can be interpreted as probabilities.\n",
        "There is an alternative view of the multinomial logit in similar spirit:\n",
        "we start from a linear model and we want to turn its ouput into something that can be interpreted as probabilities.\n",
        "\n",
        "What the multinomial is doing is similar to squashing, we want to force all individual predictions \n",
        "to be between 0 and 1. But we want them to also to sum to 1. In binary choice, this is not a problem, because we can model one of the two alternatives and set the probabiliy of the other to (1 - modeled probability). When we have more than 2 alternatives, we need to think a bit more.\n",
        "\n",
        "A natural way of getting there is through the softmax function.\n",
        "The softmax takes as input a vector of arbitrary numbers and produces as ouput a vector of the same dimension, but all outputs are between 0 and 1 and sum to 1. In mathematical notation, supposing we have a vector of size $J$:\n",
        "\n",
        "$$ \\text{softmax}: \\mathbb{R}^J \\to [0,1]^J $$\n",
        "\n",
        "The definition of softmax can be separated for each element of the input vector.\n",
        "For the element $i$ in the input vector $x$:\n",
        "\n",
        "$$ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j \\in J}e^{x_j}}$$\n",
        "\n",
        "Literally, we calculate the exponential of all input numbers, the the softmax for each element of the input is its exponential dividied by the sum of all exponentials.\n",
        "\n",
        "The function is called softmax because it will tend to produce an output of 1 in the largest element of the input, such as the indicator function for which is the maximum element in the input vector. So it is like computing the indicator function 'which element is the maximum' but more smoothly or 'fuzzy', with values inbetween 0 and 1.\n",
        "\n",
        "The softmax is used in Machine Learning for classification, so the two concepts ,multinomial logit and softmax classifiers, are deeply connected.\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPfzbfZyYOln"
      },
      "source": [
        "\n",
        "\n",
        "# Maximum likelihood\n",
        "\n",
        "We want to estimate the coeffients in our multinomial logit. We now have a vector of coefficients for each alternative.\n",
        "We can adapt the method of maximum likelihood we saw in binary choice to the situation when we have more than two options.\n",
        "\n",
        "Remember that for maximum likelihood we need to know the underlying probability distribution that generates the randomness. In the case of binary choice, it is clear that the Bernouilli is a good solution. When we move to multiple choices, then natural extension is the Multinomial distribution (for the number of trials equals 1).\n",
        "\n",
        "For a given observation, the likelihood of the model parameterized by the coefficients $\\beta$, considering $Beta$ the set of all coefficients for each alternative.\n",
        "\n",
        "$$ L( \\beta  ) = \\prod_{j \\in J} ( \\text{prob. produced by } \\beta \\text{ for the alternative } j)^{\\text{indicator of alternative } j \\text{ was observed in the data}}$$\n",
        "\n",
        "For the whole dataset, we take the product of all individual observations. Again, the logarithm transform is usually applied for practical reasons.\n",
        "\n",
        "And that is why it is called the multinomial logit.\n",
        "\n",
        "This time, maximum likelihood on the multinomial is connected to the cross-entropy loss in Machine Learning, they are two alternative points of view.\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpopgOsxYXls"
      },
      "source": [
        "\n",
        "\n",
        "#An example of likelihood where the distribution is not multinomial\n",
        "\n",
        "We have mentioned that both the Bernouilli and Multinomial distributions are very natural or fundamental candidates for underlying distributions when calculating likelihoods. We will see an example when this is not that obvious,\n",
        "to give some perspective.\n",
        "\n",
        "\n",
        "Imagine that we want to model the choice for the number of episodes that indivudals are going to watch of a particular tv show. We can think of a set of alternatives 0, 1, 2, 3... and consider the multinomial, but there might be a better distribution that describes the data in this case. In the multinomial, we are ignoring the fact that numbers have some similarities (e.g. 2 is closer to 3 than to 0). Another distribution could be applied for the likelihood, for example, the Poisson that is commonly used for counts.\n",
        "\n",
        "In practice, this not something that is clear beforehand, before trying both ideas, the Multinomial and the Poisson. There are cases where in fact the multinomial is recommended for count data, for example, Amazon has research on using the multinomial for the likelihood when predicting the sales of products, because it is difficul to find a more natural distributions.\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoEfx_qGfYh9"
      },
      "source": [
        "\n",
        "\n",
        "# Interpretation as Random Utility with a Gumbel distribtution on the random component\n",
        "\n",
        "As we have shown with the logit, the multinomial logit:\n",
        "\n",
        "\n",
        "$$ P(a | X) = \\frac{ e^{V_a}}{\\sum_{b \\in X} e^{V_b}}$$\n",
        "can be interpreted under the random utility model as a decomposition:\n",
        "\n",
        "$$U_{nj} = V_{nj} + \\varepsilon_{nj}$$\n",
        "\n",
        "with $\\varepsilon_{nj}$ are independent and identically distributed Gumbel.\n",
        "\n",
        "McFadded showed the opposite direction, that under some mild restrictions, \n",
        "the Gumbel induces the multinomial logit.\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i3ixVSwg2Y6"
      },
      "source": [
        "\n",
        "\n",
        "#Limitations of the multinomial logit\n",
        "\n",
        "The Independence of Irrelevant Alternatives, the constant odds creates an interesting paradox. There is a famous illustration of this, the Red bus / Blue bus problem.\n",
        "\n",
        "Imagine a choice situation for driving to work. The initial choice set is among car and bus. Very importantly, the buses are red.\n",
        "\n",
        "Lets say that the probabilities are 0.66 for car and 0.33 for bus.\n",
        "\n",
        "We now introduce a new alternative, a blue bus. The variables of the blue bus are the same as the red bus, therefore the predicted utility in the model is the same as for the red bus. Under I.I.A. we expect the ratio of probabilities among car and red to bus keep constant when we introduce the blue bus.\n",
        "\n",
        "Imagin that the utilities for car is $log(2)$ and for bus is $\\log(1)$, we use logarithm so when we exponentiate them in the multinomial logit,\n",
        "$\\frac{ e^{V_a}}{\\sum_{b \\in X} e^{V_b}}$ they cancel and we get some nice numbers.\n",
        "\n",
        "\n",
        "$$P(Car | \\{Car, RedBus\\}) = \\frac{2}{2 + 1} = 0.66$$\n",
        "$$P(RedBus  | \\{Car, RedBus\\}) = \\frac{1}{2 + 1} = 0.33$$\n",
        "\n",
        "when we introduce the blue bus in the choice set, because of the properties of a blue bus are the same as the red car, the color does not affect the choice, the utility for the blue bus is the same as the red bus. We compute the choice probabilties now and we get.\n",
        "\n",
        "$$P(Car |  \\{Car, Redbus, BlueBus\\}) = \\frac{2}{2 + 1 + 1} = 0.5$$\n",
        "$$P(RedBus |  \\{Car, Redbus, BlueBus\\}) = \\frac{1}{2 + 1 + 1} = 0.25$$\n",
        "\n",
        "We see that the I.I.A. holds, in both choice sets, the odds of car to red bus has kept constant.\n",
        "$$\\frac{0.66}{0.33} = 2 = \\frac{0.5}{0.25}$$\n",
        "\n",
        "However, this result can seem a bit strange or counterintuitive. Most people would say that introducing a bus of another color should not change the probability of choosing car! We can imagine even more exaggerated examples, some new alternative that there is in not relevant way different from the original. Such as buses that have been manufactured only on Tuesdays in the production chain. Why should this new alternative drive preferences from car? \n",
        "\n",
        "What is commonly said is that these similar alternatives should 'lump together'\n",
        "and share the original choice probability. Car = 0.66, RedBus=0.165, BlueBus=0.165.\n",
        "\n",
        "Attempts to solve this paradox give raise to alternative models, such as the **nested logit** which we will study in later lectures.\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBcl9ZGtmilk"
      },
      "source": [
        "# Counterpoint to the paradox\n",
        "\n",
        "Perhaps one would wonder why the multinomial logit is so popular if it exhibits\n",
        "that strange property.\n",
        "\n",
        "In fact, the counter argument to the problem often a 'properly specified model'\n",
        "should exhibit I.I.A. This means that we should capture the sources of similarity of alternatives. There are several ways:\n",
        "\n",
        " * We could directly not consider the blue bus as a new alternative\n",
        " * Are the blue buses and red buses really identical? For example, maybe they do not have the same schedule (buses cannot ocupy the same physical space at the same time!), so if we can capture these differences in our model, we will be able predict the new situation well.\n",
        " * We can consider that the attributes of the alternatives change when we introduce the new one. For example, we can specify the variable: 'number of buses of other colors' that would change when introducing the blue bus. This could keep the choice probabilities as intuitively expected.\n",
        "\n",
        "Other arguments are:\n",
        " * We actually do not know how the reaction of the decision makers will be. Maybe some car drives were waiting for the oportunity of riding a blue bus...\n",
        " * Other models that do not have this problem actually require knowing the similarities of the alternatives beforehand.\n",
        "\n",
        "**The bottom line: The multinomial logit is flexible enough, alternative models are 'tools in our toolbox' that can make the modelling easier in some situations.**\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcdHEJOUmMdH"
      },
      "source": [
        "# Validation\n",
        "\n",
        "With more than two alternatives, we can extend the accuracy metric\n",
        "and look at the confusion matrix, it can give valuabe information.\n",
        "\n",
        "| Predicted \\ Actual  | Air  | Train   | Bus  | Total |\n",
        "|---|---|---|---|--- |\n",
        "| Air  | **7**  |8   |  2 |  17|\n",
        "| Train  | 2  | **3**  |  1 |  6|\n",
        "|  Bus |  3 |  2 | **1** |  6|\n",
        "|  Total |  12|  13|  4 |  29|\n",
        "\n",
        "\n"
      ]
    }
  ]
}